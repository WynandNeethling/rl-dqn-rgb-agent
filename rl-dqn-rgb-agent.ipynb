{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import *\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractObjectInformation2(observation):\n",
    "    (rows, cols, x) = observation.shape\n",
    "    tmp = np.reshape(observation,[rows*cols*x,1], 'F')[0:rows*cols]\n",
    "    return np.reshape(tmp, [rows,cols],'C')\n",
    "\n",
    "def normalize(observation, max_value):\n",
    "    return np.array(observation)/max_value\n",
    "\n",
    "def flatten(observation):\n",
    "    return torch.from_numpy(np.array(observation).flatten()).float().unsqueeze(0)\n",
    "\n",
    "def preprocess(observation):\n",
    "    return flatten(normalize(extractObjectInformation2(observation), 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action function & Optimise model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, numActions, steps_done, start_epsilon, stop_epsilon, decay_rate):\n",
    "    sample = random.random()\n",
    "    eps_threshold = stop_epsilon+(start_epsilon-stop_epsilon)*math.exp(-1. * steps_done / decay_rate)\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].unsqueeze(0)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(numActions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(memory, policy_net, target_net, optimizer, criterion, gamma, batch_size, pretrain_length):\n",
    "    if len(memory) < pretrain_length:\n",
    "        return\n",
    "\n",
    "    # Sample mini-batch\n",
    "    experience = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*experience))\n",
    "\n",
    "    # Calculate action-values using policy network\n",
    "    state_batch = torch.cat(batch.currentState)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calculate TD-targets using target network\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    non_final_next_states = torch.cat([s for s in batch.nextState if s is not None])\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.nextState)), device=device, dtype=torch.bool)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    TDtargets = (next_state_values * gamma) + reward_batch\n",
    "    TDerrors = TDtargets.unsqueeze(1) - state_action_values\n",
    "\n",
    "    # Calculate loos\n",
    "    loss = criterion(state_action_values, TDtargets.unsqueeze(1))\n",
    "    # Make gradient descrent step and update policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputSize, numActions, hiddenLayerSize=(512, 256)):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, hiddenLayerSize[0])\n",
    "        self.fc2 = nn.Linear(hiddenLayerSize[0], hiddenLayerSize[1])\n",
    "        self.fc3 = nn.Linear(hiddenLayerSize[1], numActions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "Transition = namedtuple('Transition',('currentState', 'action', 'nextState', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Our algorithm is summarized below:\n",
    "<br>\n",
    "* Initialize the weights of the policy network and target networks\n",
    "* Initialize the environment\n",
    "* Set the decay rate (that will use to calculate the $\\epsilon_{\\textrm{threshold}}$)\n",
    "* Set fixed Q-value target update threshold\n",
    "* Set total training steps to 0\n",
    "* Create replay memory $\\mathcal{D}$\n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Set step to 0\n",
    "    * Make new episode\n",
    "    * Observe the first state $s$\n",
    "    <br><br>\n",
    "    * **While** {(not done) and (step < max_steps)} **do**:\n",
    "        * With $\\epsilon$ select a random action $a$, otherwise select $a = \\mathrm{argmax}_a Q(s,a)$\n",
    "        * Increment the total training steps \n",
    "        * Execute action $a_t$ in environment, observe reward $r$ and new state $s'$\n",
    "        * Store transition $<s, a, r, s'>$ in replay memory $\\mathcal{D}$\n",
    "        * **If** size($\\mathcal{D}$) >= mini-batch size $N$: \n",
    "            * Sample random mini-batch from $\\mathcal{D}$: $<s_i, a_i, r_i, s_i'>$ with $i=1\\ldots N$\n",
    "            * Set TD-target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "            * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * **If** total_steps > fixed Q-value target update threshold:\n",
    "            * Copy parameters from policy network to target network\n",
    "    * **endwhile**\n",
    "    <br><br>\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy_net, target_net, memory, optimizer, criterion, gamma, batch_size, target_update, start_epsilon, stop_epsilon, decay_rate, episodes, pretrain_length, numActions):\n",
    "    steps_done = 0\n",
    "\n",
    "    for e in range(episodes):\n",
    "        currentObs, _ = env.reset()\n",
    "        currentState = preprocess(currentObs)\n",
    "        \n",
    "        for i in range(0, env.max_steps):\n",
    "            # Choose an action\n",
    "            action = select_action(currentState, policy_net, numActions, steps_done, start_epsilon, stop_epsilon, decay_rate)\n",
    "            a = action.item()\n",
    "            steps_done += 1\n",
    "            \n",
    "            # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "            # 'done' indicate if the termination state was reached\n",
    "            obs, reward, done, truncated, info = env.step(a)\n",
    "    \n",
    "            if (done or truncated):\n",
    "                nextState = None\n",
    "            else:\n",
    "                nextState = preprocess(obs)\n",
    "            \n",
    "            # Store the transition <s,a,r,s'> in the replay memory\n",
    "            reward = torch.tensor([reward], device = device)\n",
    "            memory.push(currentState, action, nextState, reward)\n",
    "\n",
    "            # Move to the next state          \n",
    "            currentState = nextState\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network) by\n",
    "            # sample a mini-batch and train the model using the sampled mini-batch\n",
    "            optimize_model(memory, policy_net, target_net, optimizer, criterion, gamma, batch_size, pretrain_length)\n",
    "            \n",
    "            # If the target update threshold is reached, update the target network, \n",
    "            # copying all weights and biases in the policy network\n",
    "            if steps_done % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if (done or truncated):\n",
    "                if (done):\n",
    "                    print('Finished episode successfully taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "                else:\n",
    "                    print('Truncated episode taking %d steps and receiving reward %f' % (env.step_count, reward))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS \n",
    "numActions = 3\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "alpha = 0.0002               # learning_rate\n",
    "episodes = 5000              # Total episodes for training\n",
    "batch_size = 128             # Neural network batch size\n",
    "target_update = 5000        # Number of episodes between updating target network\n",
    "gamma = 0.90\n",
    "\n",
    "start_epsilon = 1.0          # exploration probability at start\n",
    "stop_epsilon = 0.01          # minimum exploration probability \n",
    "decay_rate = 20000           # exponential decay rate for exploration prob\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size # Number of experiences stored in the Memory when initialized for the first time\n",
    "memorySize = 500000          # Number of experiences the Memory can keep - 500000\n",
    "\n",
    "### TESTING HYPERPARAMETERS\n",
    "evalEpisodes = 1000\n",
    "train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "policy_net = DQN(49, numActions, (128,128))\n",
    "target_net = DQN(49, numActions, (128,128))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "memory = ReplayMemory(memorySize)\n",
    "\n",
    "train(env, policy_net, target_net, memory, optimizer, criterion, gamma, batch_size, target_update, start_epsilon, stop_epsilon, decay_rate, episodes, pretrain_length, numActions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
